{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to load the models and set them up for further analyses\n",
    "\n",
    "### Loading the models\n",
    "The pipeline has two functions to load the models used in the analyses: \n",
    "1. *get_reference_model* is used to load the model of the organism that has to be engineered\n",
    "2. *get_universal_main* is used to load the universal model, which is one of the 5 versions of BiGG database contained in CarveMe [Gitlab repostiory](https://github.com/cdanielmachado/carveme/tree/master/carveme/data/generated).  \n",
    "\n",
    "    - The reactions from all bacteria species in BiGG\n",
    "    - A special universal reaction database with reactions specific for Gram negative and one for Gram positive bacetia\n",
    "    - Another special reaction database specific for archaea\n",
    "    - Finally a set of reactions that inlcudes specific cyanobacteria reactions\n",
    "    \n",
    "For more information on the different universal models see CarveMe pubblication of Machado et al 2018 [1] \n",
    "\n",
    "### Preparing the models\n",
    "The *main* function is used to read the information in the input file (look at tutorial n° 0) and check for the presence of exchange reactions and transporters in the model of the organism of interest. Additionally it adds to the universal model any reactions that the user might have indicated as possibly involved in the conversion/production pathway.  \n",
    "\n",
    "\n",
    "The inport statement should always import COBRApy[2], since it is useful to complement the pipeline's specific functions with the wide variaty of functions of COBRApy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import statemets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from pipeline_package import import_models, input_parser\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "data_repo = \"../inputs\"\n",
    "model = import_models.get_reference_model(data_repo, '../inputs/ecoli_tutorial.csv')\n",
    "universal = import_models.get_universal_main(data_repo, '../inputs/ecoli_tutorial.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "input_parser.parser('../inputs/ecoli_tutorial.csv', universal, model)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'pipeline_package.input_parser' has no attribute 'parser'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5b33da927197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../inputs/ecoli_tutorial.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniversal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pipeline_package.input_parser' has no attribute 'parser'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The printed output above can be unserstood better looking at the example input file *ecoli_tutorial.csv* that can be found in the inputs directory. \n",
    "\n",
    "* There is not transporter of methane in E. coli model iML1515. Thus, the pipeline adds the reaction included in the input file \n",
    "* ALCD1 reaction (methanole dehydrogenase) is already in the model, so the pipeline recognize it and does not add it to the universal\n",
    "* There are no methane oxidizing reactions in the BiGG database, therefore the pipeline adds them to the universal reaction model.\n",
    "\n",
    "![input_main](./images/main.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Analysis for reactions addition allowing growth on uncommon substrates (everything until consumption, maybe look for putida xyl notebooks for final dict and scores if I did it)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**References**\n",
    "\n",
    "1. D. Machado, S. Andrejev, M. Tramontano, and K. R. Patil, “Fast automated reconstruction of genome-scale metabolic models for microbial species and communities,” Nucleic Acids Res., vol. 46, no. 15, pp. 7542–7553, 2018, [doi: 10.1093/nar/gky537](https://academic.oup.com/nar/article/46/15/7542/5042022)\n",
    "2. A. Ebrahim, J. A. Lerman, B. O. Palsson, and D. R. Hyduke, “COBRApy: COnstraints-Based Reconstruction and Analysis for Python,” BMC Syst. Biol., vol. 7, no. 1, p. 74, Aug. 2013, doi: 10.1186/1752-0509-7-74."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}